{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pprint\n",
    "import textwrap\n",
    "import json\n",
    "\n",
    "from math import sqrt, log\n",
    "from hashlib import sha1\n",
    "from graphviz import Digraph\n",
    "from credentials import ID, SECRET\n",
    "from praw.models import MoreComments as more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=ID,\n",
    "    client_secret=SECRET,\n",
    "    user_agent=\"The Rhetor Project\"\n",
    ")\n",
    "\n",
    "ID = \"17llow9\"\n",
    "post = reddit.submission(id=ID)\n",
    "topic = post.title[5:]\n",
    "desc = post.selftext\n",
    "comments = post.comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "# nltk.download()\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from colour import Color\n",
    "\n",
    "# (Inadequate) sentiment analysis\n",
    "def sentiment(reply):\n",
    "\tsents = sent_tokenize(reply)[0]\n",
    "\tSIA = SentimentIntensityAnalyzer()\n",
    "\treturn SIA.polarity_scores(sents)[\"compound\"]\n",
    "\n",
    "def clean(raw):\n",
    "\tparsed = soup(raw)\n",
    "\n",
    "\t# Remove all <blockquotes>\n",
    "\tfor b in parsed.find_all('blockquote'):\n",
    "\t\tb.extract()\n",
    "\t\n",
    "\treturn parsed.get_text()\n",
    "\n",
    "lo, hi = Color(\"#ffffff\"), Color(\"#ffea80\")\n",
    "gradient = list(lo.range_to(hi, 101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned = [0, 0, 0]\n",
    "largest = 0\n",
    "pairs = []\n",
    "\n",
    "def process(parent, comment):\n",
    "    cur = {\n",
    "        \"content\": clean(comment.body_html),\n",
    "        \"id\": comment.id,\n",
    "        \"name\": sha1(f\"{comment.author.name}\".encode()).hexdigest(),\n",
    "        \"depth\": comment.depth+1,\n",
    "        \"votes\": 0 if comment.score_hidden else abs(comment.score),\n",
    "        \"ranked\": True if comment.author_flair_text else False,\n",
    "        \"spicy\": bool(comment.controversiality),\n",
    "        \"score\": 0,\n",
    "        \"threadScore\": 0,\n",
    "        \"below\": False, # max # layers below\n",
    "        \"agree\": None,\n",
    "        \"replies\": []\n",
    "    }\n",
    "    \n",
    "    # Prune\n",
    "    if not cur[\"spicy\"] and cur[\"votes\"] < 3:\n",
    "        pruned[0] += 1; return\n",
    "    # if len(cur[\"content\"]) < 50:\n",
    "    wordCount = len(word_tokenize(cur[\"content\"]))\n",
    "    if wordCount < 15: pruned[1] += 1; return\n",
    "\n",
    "    maxBelow = 0\n",
    "    for r in comment.replies:\n",
    "        if isinstance(r, more) or not r.author: continue\n",
    "        replyThread = process(cur, r)  # Recurse\n",
    "        if replyThread: \n",
    "            cur[\"replies\"].append(replyThread)\n",
    "            cur[\"threadScore\"] += replyThread[\"score\"]\n",
    "            if replyThread[\"below\"] is not False:\n",
    "                if replyThread[\"below\"] > maxBelow: maxBelow = replyThread[\"below\"]\n",
    "\n",
    "    # Depth/below calc\n",
    "    if not cur[\"replies\"]:\n",
    "        cur[\"below\"] = 0 # False -> 0: hit a leaf\n",
    "        if cur[\"depth\"] < 3: \n",
    "            pruned[2] += 1; return\n",
    "    else:\n",
    "        cur[\"below\"] = maxBelow + 1\n",
    "\n",
    "    # Score calculation\n",
    "    S = 1.2 if cur[\"spicy\"] else 1   # boost controvserial comment scores\n",
    "    R = 1.4 if cur[\"ranked\"] else 1  # favor \"seasoned\" commenters\n",
    "    L = sqrt((wordCount-10)/5)       # encourage longer comments\n",
    "    D = cur[\"depth\"]                 # boost deep comment scores\n",
    "    B = (cur[\"below\"]+.5)/2          # devalue no/shallow subthreads\n",
    "\n",
    "    cur[\"score\"] = S*R*L*D*B*cur[\"votes\"]\n",
    "    cur[\"threadScore\"] += cur[\"score\"]\n",
    "    global largest\n",
    "    if cur[\"threadScore\"] > largest: largest = cur[\"threadScore\"]\n",
    "\n",
    "    # (Dis)agreement\n",
    "    # sen = sentiment(cur[\"content\"])\n",
    "    # cur[\"agree\"] = False if sen < -0.7 else (True if sen > 0.9 else None)\n",
    "\n",
    "    # Save pairs for classification en masse\n",
    "    pairs.append({\n",
    "        \"id\": cur[\"id\"],\n",
    "        \"pre_text\": topic if not parent else parent[\"content\"], \n",
    "        # \"pre_text\": topic if not parent else ''.join(sent_tokenize(parent[\"content\"])[:2]),\n",
    "        \"con_text\": cur[\"content\"],\n",
    "        # \"con_text\": ''.join(sent_tokenize(cur[\"content\"])[:2]),\n",
    "    })\n",
    "\n",
    "    return cur \n",
    "\n",
    "\n",
    "def traverse(comments):\n",
    "    threads = []\n",
    "    for c in comments: # Each TLC\n",
    "        if isinstance(c, more) or c.stickied: continue\n",
    "        if c.author is None: continue\n",
    "        thread = process({}, c)\n",
    "        if thread: \n",
    "            threads.append(thread)\n",
    "            # print(f\"{thread['threadScore']:0.2f}\")\n",
    "    return threads\n",
    "\n",
    "tree = {\n",
    "    \"root\": {\n",
    "        \"id\": ID,\n",
    "        \"content\": topic,\n",
    "        \"replies\": traverse(comments)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"blend_warmup\": True,\n",
    "    \"blend_type\": \"no\",\n",
    "    \"blend_rate\": [0] * 100,\n",
    "    \"train_blend_warmup\": False,\n",
    "    \"n_epochs_blend_only\": 5,\n",
    "\n",
    "    \"debug_mode\": False, #True,\n",
    "\n",
    "    \"device\": \"cuda\",\n",
    "    \"pt_model\": \"bert-base-uncased\",\n",
    "    \"n_trials\": 1,\n",
    "    \"n_epochs\": 50,\n",
    "    \"learn_rate\": 1e-5,\n",
    "    \"epsilon\": 1e-8,\n",
    "    \"pivot_metric\": \"auc\",\n",
    "    \"early_stop\": 3,\n",
    "    \"tasks\": {\n",
    "        \"main\": {\n",
    "            \"classes\": [\"sup\", \"att\", \"neu\"] \n",
    "        },\n",
    "        \"nli\": {\n",
    "            \"data_paths\": [\"data/rsc/mnli-org.csv\",\n",
    "                           \"data/rsc/antsyn-nli.csv\"],\n",
    "            \"classes\": [\"ent\", \"con\", \"neu\"],\n",
    "        },\n",
    "        \"senti\": {\n",
    "            \"data_paths\": [\"data/rsc/senti-irish.csv\",\n",
    "                           \"data/rsc/senti-ldong.csv\",\n",
    "                           \"data/rsc/senti-mm.csv\",\n",
    "                           \"data/rsc/senti-sem17.csv\",\n",
    "                           \"data/rsc/senti-norm.csv\"],\n",
    "            \"classes\": [\"pos\", \"neg\", \"neu\"],\n",
    "        },\n",
    "        \"causal\": {\n",
    "            \"data_paths\": [\"data/rsc/because-causal.csv\",\n",
    "                           \"data/rsc/conet-causal.csv\",\n",
    "                           \"data/rsc/pdtb-i-causal.csv\",\n",
    "                           \"data/rsc/wiqa-causal.csv\"],\n",
    "            \"classes\": [\"cause\", \"obstruct\", \"precede\", \"sync\", \"else\"],\n",
    "        },\n",
    "        \"normarg_polar\": {\n",
    "            \"data_path\": \"data/rsc/normarg.csv\",\n",
    "            \"classes\": [\"consist\", \"contrast\"],\n",
    "        },\n",
    "        \"normarg_jtype\": {\n",
    "            \"data_path\": \"data/rsc/normarg.csv\",\n",
    "            \"classes\": [\"norm\", \"conseq\"],\n",
    "        },\n",
    "        \"normarg_senti\": {\n",
    "            \"data_path\": \"data/rsc/normarg.csv\",\n",
    "            \"classes\": [\"positive\", \"negative\"],\n",
    "            \"con_classes\": [\"advocate\", \"object\"],\n",
    "        },\n",
    " \n",
    "    },\n",
    "    \"max_n_batch_tokens\": 512*5,\n",
    "    \"max_batch_size\": 6,\n",
    "    \"data_dir\": None,\n",
    "    \"logs_dir\": None,\n",
    "    \"save_model\": False,\n",
    "    \"models_dir\": \"models\",\n",
    "    \"rel2label\": {\n",
    "            \"1\": 0,  # support\n",
    "            \"-1\": 1,  # attack\n",
    "            \"0\": 2  # neutral\n",
    "    }\n",
    "}\n",
    "\n",
    "tasks = [\"main\"] + [\"nli\", \"senti\", \"normarg\"]\n",
    "\n",
    "\n",
    "cfg[\"task2classes\"] = {}\n",
    "for task in tasks:\n",
    "    if task == \"normarg\":\n",
    "        cfg[\"task2classes\"][\"normarg_polar\"] = cfg[\"tasks\"][\"normarg_polar\"][\"classes\"]\n",
    "        cfg[\"task2classes\"][\"normarg_jtype\"] = cfg[\"tasks\"][\"normarg_jtype\"][\"classes\"]\n",
    "        cfg[\"task2classes\"][\"normarg_norm_senti\"] = cfg[\"tasks\"][\"normarg_senti\"][\"classes\"]\n",
    "        cfg[\"task2classes\"][\"normarg_conseq_senti\"] = cfg[\"tasks\"][\"normarg_senti\"][\"classes\"]\n",
    "    else:\n",
    "        cfg[\"task2classes\"][task] = cfg[\"tasks\"][task][\"classes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model...\n",
      "Loading data...\n",
      "Classifying 92 replies over 17 batches:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:00<00:00, 18.40it/s]\n"
     ]
    }
   ],
   "source": [
    "from csv_utils import *\n",
    "from models import *\n",
    "import os\n",
    "\n",
    "MODEL = 'trained.model'\n",
    "\n",
    "assert os.path.exists(MODEL), f\"Model not found: {MODEL}\"\n",
    "# Prepare the trainer\n",
    "trainer = Trainer(cfg)\n",
    "\n",
    "# Warmup with blend tasks\n",
    "init_model = torch.load(MODEL, map_location='cuda')\n",
    "trainer.init_model(init_model)\n",
    "trainer.model = trainer.model.to('cuda')\n",
    "\n",
    "# Prediction\n",
    "keys = [\"id\", \"con_text\", \"pre_text\", \"label_prob\", \"label_pred\"]\n",
    "\n",
    "print(f\"Loading data...\")\n",
    "batches = trainer.make_batches(pairs, [\"pre\", \"con\"])\n",
    "\n",
    "print(f\"Classifying {sum(len(b[\"id\"]) for b in batches)} replies over {len(batches)} batches:\")\n",
    "\n",
    "trainer.run_epoch(batches, \"predict\", \"main\")\n",
    "\n",
    "# for batch in batches:\n",
    "#     for i, id in enumerate(batch[\"id\"]):\n",
    "#         print(batch[\"pre_text\"][i][:20],'\\t',batch[\"con_text\"][i][:20],'\\t',batch[\"label_prob\"][i],'\\t',batch[\"label_pred\"][i])\n",
    "res = {ID: [b[\"label_pred\"][i], b[\"label_prob\"][i]] for b in batches for i, ID in enumerate(b[\"id\"])}\n",
    "# for ID in res:\n",
    "#     print(f\"{ID}: {res[ID]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = Digraph(format='png')\n",
    "\n",
    "def build(parID, cur):\n",
    "    # print(res[cur[\"id\"]])\n",
    "    g = int(25*(cur[\"threadScore\"]/largest*100)**(.25)+0)\n",
    "    \n",
    "    pred = res[cur[\"id\"]][0]; prob = res[cur[\"id\"]][1]\n",
    "    prob = [round(x,3) for x in prob]\n",
    "    label = f\"{cur['threadScore']:.2f} ({cur['score']:.2f})\\n{prob}\\n{textwrap.fill(cur[\"content\"], 25)[:200]}{'...' if len(cur[\"content\"]) > 200 else ''}\"\n",
    "    vis.node(cur[\"id\"], label=label, shape='box', style='filled', fillcolor=gradient[g].hex_l)\n",
    "\n",
    "    # cur[\"agree\"] = None if pred == 2 else not bool(pred)\n",
    "    cur[\"agree\"] = True if (prob[0] > prob[1]) else False\n",
    "    edge_color = 'black' if cur[\"agree\"] is None else ('green' if cur[\"agree\"] else 'red')\n",
    "    edge_label = '?' if cur[\"agree\"] is None else ('Agree' if cur[\"agree\"] else 'Disagree')\n",
    "\n",
    "    vis.edge(parID, cur[\"id\"], label=edge_label, color=edge_color, fontcolor=edge_color)\n",
    "\n",
    "    # Recurse for the replies\n",
    "    for r in cur[\"replies\"]:\n",
    "        build(cur[\"id\"], r)\n",
    "\n",
    "# Add a root node to the graph with the topic\n",
    "root = tree[\"root\"]\n",
    "vis.node(root[\"id\"], label=topic, shape='box')\n",
    "\n",
    "for r in root['replies']:\n",
    "    build(root[\"id\"], r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned comments:\n",
      "\t42 with low engagement\n",
      "\t27 were too short\n",
      "\t47 from shallow threads\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tree.png'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F: Not sharing \"/usr/share/icons\" with sandbox: Path \"/usr\" is reserved by Flatpak\n",
      "\n",
      "(loupe:2): GLib-GObject-CRITICAL **: 21:19:22.399: g_object_weak_unref: couldn't find weak ref 0x7ff5c8352fb0((nil))\n",
      "\n",
      "(loupe:2): GLib-GObject-CRITICAL **: 21:19:43.768: g_object_weak_unref: couldn't find weak ref 0x7ff5c8352fb0((nil))\n"
     ]
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4, sort_dicts=False)\n",
    "# pp.pprint(tree)\n",
    "json.dump(tree, open(\"tree.json\", \"w\"), indent=4, sort_keys=False)\n",
    "\n",
    "print(\n",
    "\t\"Pruned comments:\\n\"\n",
    "\tf\"\\t{pruned[0]} with low engagement\\n\"\n",
    "\tf\"\\t{pruned[1]} were too short\\n\"\n",
    "\tf\"\\t{pruned[2]} from shallow threads\\n\"\n",
    ")\n",
    "\n",
    "vis.render('tree', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4332.052884826169\n",
      "3474.562500742937\n",
      "847.1289219104086\n",
      "733.8629874438277\n",
      "635.6679389227189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'script.png'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F: Not sharing \"/usr/share/icons\" with sandbox: Path \"/usr\" is reserved by Flatpak\n",
      "\n",
      "(loupe:2): Gtk-WARNING **: 21:20:57.574: Trying to snapshot GtkGizmo 0x55c98c72fff0 without a current allocation\n",
      "\n",
      "(loupe:2): Gtk-WARNING **: 21:36:46.717: Trying to snapshot GtkGizmo 0x55c98c4e3210 without a current allocation\n"
     ]
    }
   ],
   "source": [
    "script = tree\n",
    "vis2 = Digraph(format='png')\n",
    "\n",
    "def build(parID, cur):\n",
    "    if cur[\"agree\"]: return\n",
    "    cur[\"replies\"].sort(key=lambda x: x[\"threadScore\"], reverse=True)\n",
    "    g = int(25*(cur[\"threadScore\"]/largest*100)**(.25)+0)\n",
    "    \n",
    "    label = f\"{textwrap.fill(cur[\"content\"], 50)[:1000]}{'...' if len(cur[\"content\"]) > 1000 else ''}\"\n",
    "    vis2.node(cur[\"id\"], label=label, shape='box', style='filled', fillcolor=gradient[g].hex_l)\n",
    "\n",
    "    edge_color = 'black' if cur[\"agree\"] is None else ('green' if cur[\"agree\"] else 'red')\n",
    "    edge_label = '?' if cur[\"agree\"] is None else ('Agree' if cur[\"agree\"] else 'Disagree')\n",
    "\n",
    "    vis2.edge(parID, cur[\"id\"], label=edge_label, color=edge_color, fontcolor=edge_color)\n",
    "\n",
    "    # Recurse for the replies\n",
    "    if cur[\"replies\"]:\n",
    "        build(cur[\"id\"], cur[\"replies\"][0])\n",
    "\n",
    "\n",
    "vis2.node(root[\"id\"], label=topic, shape='box')\n",
    "root = script[\"root\"]\n",
    "root[\"replies\"].sort(key=lambda x: x[\"threadScore\"], reverse=True)\n",
    "for tlc in root[\"replies\"][:5]: # explore top 3 threads\n",
    "    build(root[\"id\"], tlc)\n",
    "    print(tlc[\"threadScore\"])\n",
    "\n",
    "\n",
    "json.dump(script, open(\"script.json\", \"w\"), indent=4, sort_keys=False)\n",
    "\n",
    "vis2.render('script', view=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

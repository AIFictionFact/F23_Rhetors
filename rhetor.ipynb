{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pprint\n",
    "import textwrap\n",
    "import json\n",
    "\n",
    "from math import sqrt, log\n",
    "from hashlib import sha1\n",
    "from graphviz import Digraph\n",
    "from credentials import ID, SECRET # you need your won credentials.py to make this work: Bren used his own reddit credentials\n",
    "from praw.models import MoreComments as more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prawing the resubreddit threads\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=ID,\n",
    "    client_secret=SECRET,\n",
    "    user_agent=\"The Rhetor Project\"\n",
    ")\n",
    "\n",
    "ID = \"17llow9\"\n",
    "post = reddit.submission(id=ID)\n",
    "topic = post.title[5:]\n",
    "desc = post.selftext\n",
    "comments = post.comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data preprocessing\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "# nltk.download()\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from colour import Color\n",
    "\n",
    "# (Inadequate) sentiment analysis (DEPRECATED)\n",
    "def sentiment(reply):\n",
    "\tsents = sent_tokenize(reply)[0]\n",
    "\tSIA = SentimentIntensityAnalyzer()\n",
    "\treturn SIA.polarity_scores(sents)[\"compound\"]\n",
    "\n",
    "def clean(raw):\n",
    "\tparsed = soup(raw)\n",
    "\n",
    "\treplace = {\n",
    "\t\t\"\\u200b\": \" \",\n",
    "\t\t\"\\u2019\": \"'\",\n",
    "\t\t\"\\u201c\": \"\\\"\",\n",
    "\t\t\"\\u201d\": \"\\\"\",\n",
    "\t}\n",
    "\n",
    "\t# Remove all <blockquotes> and <a> tags\n",
    "\tfor b in parsed.find_all('blockquote'):\n",
    "\t\tb.extract()\n",
    "\t\n",
    "\tfor a in parsed.find_all('a', href=True):\n",
    "\t\ttext = a.get_text()\n",
    "\t\tif '://' in text: a.replace_with('')\n",
    "\t\telse: a.replace_with(text)\n",
    "\n",
    "\tplain = parsed.get_text()\n",
    "\tfor k, v in replace.items(): plain = plain.replace(k, v)\n",
    "\n",
    "\treturn plain.strip()\n",
    "\n",
    "lo, hi = Color(\"#ffffff\"), Color(\"#ffea80\")\n",
    "gradient = list(lo.range_to(hi, 101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning reddit comments accoding to a variety of criteria\n",
    "# Details in the \"Score calculation\" section below\n",
    "\n",
    "pruned = [0, 0, 0]\n",
    "largest = 0\n",
    "pairs = []\n",
    "\n",
    "def process(parent, comment):\n",
    "    cur = {\n",
    "        \"content\": clean(comment.body_html),\n",
    "        \"id\": comment.id,\n",
    "        \"name\": sha1(f\"{comment.author.name}\".encode()).hexdigest(),\n",
    "        \"depth\": comment.depth+1,\n",
    "        \"votes\": 0 if comment.score_hidden else abs(comment.score),\n",
    "        \"ranked\": True if comment.author_flair_text else False,\n",
    "        \"spicy\": bool(comment.controversiality),\n",
    "        \"score\": 0,\n",
    "        \"threadScore\": 0,\n",
    "        \"below\": False, # max # layers below\n",
    "        \"agree\": None,\n",
    "        \"replies\": []\n",
    "    }\n",
    "    \n",
    "    # Prune\n",
    "    if not cur[\"spicy\"] and cur[\"votes\"] < 3:\n",
    "        pruned[0] += 1; return\n",
    "    # if len(cur[\"content\"]) < 50:\n",
    "    wordCount = len(word_tokenize(cur[\"content\"]))\n",
    "    if wordCount < 15: pruned[1] += 1; return\n",
    "\n",
    "    maxBelow = 0\n",
    "    for r in comment.replies:\n",
    "        if isinstance(r, more) or not r.author: continue\n",
    "        replyThread = process(cur, r)  # Recurse\n",
    "        if replyThread: \n",
    "            cur[\"replies\"].append(replyThread)\n",
    "            cur[\"threadScore\"] += replyThread[\"score\"]\n",
    "            if replyThread[\"below\"] is not False:\n",
    "                if replyThread[\"below\"] > maxBelow: maxBelow = replyThread[\"below\"]\n",
    "\n",
    "    # Depth/below calc\n",
    "    if not cur[\"replies\"]:\n",
    "        cur[\"below\"] = 0 # False -> 0: hit a leaf\n",
    "        if cur[\"depth\"] < 3: \n",
    "            pruned[2] += 1; return\n",
    "    else:\n",
    "        cur[\"below\"] = maxBelow + 1\n",
    "\n",
    "    # Score calculation\n",
    "    S = 1.2 if cur[\"spicy\"] else 1   # boost controvserial comment scores\n",
    "    R = 1.4 if cur[\"ranked\"] else 1  # favor \"seasoned\" commenters\n",
    "    L = sqrt((wordCount-10)/5)       # encourage longer comments\n",
    "    D = cur[\"depth\"]                 # boost deep comment scores\n",
    "    B = (cur[\"below\"]+.5)/2          # devalue no/shallow subthreads\n",
    "\n",
    "    cur[\"score\"] = S*R*L*D*B*cur[\"votes\"]\n",
    "    cur[\"threadScore\"] += cur[\"score\"]\n",
    "    global largest\n",
    "    if cur[\"threadScore\"] > largest: largest = cur[\"threadScore\"]\n",
    "\n",
    "    # (Dis)agreement\n",
    "    # sen = sentiment(cur[\"content\"])\n",
    "    # cur[\"agree\"] = False if sen < -0.7 else (True if sen > 0.9 else None)\n",
    "\n",
    "    # Save pairs for classification en masse\n",
    "    pairs.append({\n",
    "        \"id\": cur[\"id\"],\n",
    "        \"pre_text\": topic if not parent else parent[\"content\"], \n",
    "        # \"pre_text\": topic if not parent else ''.join(sent_tokenize(parent[\"content\"])[:2]),\n",
    "        \"con_text\": cur[\"content\"],\n",
    "        # \"con_text\": ''.join(sent_tokenize(cur[\"content\"])[:2]),\n",
    "    })\n",
    "\n",
    "    return cur \n",
    "\n",
    "# Traverses the threads in a tree-like manner and pre-process the comments\n",
    "def traverse(comments):\n",
    "    threads = []\n",
    "    for c in comments: # Each TLC\n",
    "        if isinstance(c, more) or c.stickied: continue\n",
    "        if c.author is None: continue\n",
    "        thread = process({}, c)\n",
    "        if thread: \n",
    "            threads.append(thread)\n",
    "            # print(f\"{thread['threadScore']:0.2f}\")\n",
    "    return threads\n",
    "\n",
    "# Pre-processed threads tree\n",
    "tree = {\n",
    "    \"root\": {\n",
    "        \"id\": ID,\n",
    "        \"content\": topic,\n",
    "        \"replies\": traverse(comments)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS BLOCK IS DEPRECATED\n",
    "# This was our attempt to apply a summarizer to these pruned comments\n",
    "\n",
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"stevied67/pegasus-subreddit-comments-summarizer\", device = 0)\n",
    "summPairs = []; summIn = [] \n",
    "\n",
    "for p in pairs: # Summarizer wants a flat list of pre/con pairs\n",
    "     summIn.append(p[\"pre_text\"])\n",
    "     summIn.append(p[\"con_text\"])\n",
    "     # print(p[\"pre_text\"], p[\"con_text\"], \"\\n\\n\")\n",
    "\n",
    "print(\"Summarizing content...\")\n",
    "summOut = summarizer(summIn, max_length = 32)\n",
    "print(\"Done\")\n",
    "\n",
    "\n",
    "# i = 0\n",
    "# for p in pairs:\n",
    "#     summPairs.append({\n",
    "#         \"id\": p[\"id\"],\n",
    "#         \"pre_text\": summOut[i]['summary_text'],\n",
    "#         \"con_text\": summOut[i+1]['summary_text'],\n",
    "#     })\n",
    "#     i += 2\n",
    "\n",
    "i = 0\n",
    "summs = {}\n",
    "for p in pairs:\n",
    "    summs[p['id']] = summOut[i+1]['summary_text'],\n",
    "    i += 2\n",
    "\n",
    "# for p in summPairs:\n",
    "#      print(p[\"pre_text\"], p[\"con_text\"], \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS BLOCK IS DEPRECATED\n",
    "# This was our attempt to apply the logbert model for inference\n",
    "# Original code from here: https://github.com/yohanjo/tacl_arg_rel\n",
    "\n",
    "cfg = {\n",
    "    \"blend_warmup\": True,\n",
    "    \"blend_type\": \"no\",\n",
    "    \"blend_rate\": [0] * 100,\n",
    "    \"train_blend_warmup\": False,\n",
    "    \"n_epochs_blend_only\": 5,\n",
    "\n",
    "    \"debug_mode\": False, #True,\n",
    "\n",
    "    \"device\": \"cuda\",\n",
    "    \"pt_model\": \"bert-base-uncased\",\n",
    "    \"n_trials\": 1,\n",
    "    \"n_epochs\": 50,\n",
    "    \"learn_rate\": 1e-5,\n",
    "    \"epsilon\": 1e-8,\n",
    "    \"pivot_metric\": \"auc\",\n",
    "    \"early_stop\": 3,\n",
    "    \"tasks\": {\n",
    "        \"main\": {\n",
    "            \"classes\": [\"sup\", \"att\", \"neu\"] \n",
    "        },\n",
    "        \"nli\": {\n",
    "            \"data_paths\": [\"data/rsc/mnli-org.csv\",\n",
    "                           \"data/rsc/antsyn-nli.csv\"],\n",
    "            \"classes\": [\"ent\", \"con\", \"neu\"],\n",
    "        },\n",
    "        \"senti\": {\n",
    "            \"data_paths\": [\"data/rsc/senti-irish.csv\",\n",
    "                           \"data/rsc/senti-ldong.csv\",\n",
    "                           \"data/rsc/senti-mm.csv\",\n",
    "                           \"data/rsc/senti-sem17.csv\",\n",
    "                           \"data/rsc/senti-norm.csv\"],\n",
    "            \"classes\": [\"pos\", \"neg\", \"neu\"],\n",
    "        },\n",
    "        \"causal\": {\n",
    "            \"data_paths\": [\"data/rsc/because-causal.csv\",\n",
    "                           \"data/rsc/conet-causal.csv\",\n",
    "                           \"data/rsc/pdtb-i-causal.csv\",\n",
    "                           \"data/rsc/wiqa-causal.csv\"],\n",
    "            \"classes\": [\"cause\", \"obstruct\", \"precede\", \"sync\", \"else\"],\n",
    "        },\n",
    "        \"normarg_polar\": {\n",
    "            \"data_path\": \"data/rsc/normarg.csv\",\n",
    "            \"classes\": [\"consist\", \"contrast\"],\n",
    "        },\n",
    "        \"normarg_jtype\": {\n",
    "            \"data_path\": \"data/rsc/normarg.csv\",\n",
    "            \"classes\": [\"norm\", \"conseq\"],\n",
    "        },\n",
    "        \"normarg_senti\": {\n",
    "            \"data_path\": \"data/rsc/normarg.csv\",\n",
    "            \"classes\": [\"positive\", \"negative\"],\n",
    "            \"con_classes\": [\"advocate\", \"object\"],\n",
    "        },\n",
    " \n",
    "    },\n",
    "    \"max_n_batch_tokens\": 512*5,\n",
    "    \"max_batch_size\": 6,\n",
    "    \"data_dir\": None,\n",
    "    \"logs_dir\": None,\n",
    "    \"save_model\": False,\n",
    "    \"models_dir\": \"models\",\n",
    "    \"rel2label\": {\n",
    "            \"1\": 0,  # support\n",
    "            \"-1\": 1,  # attack\n",
    "            \"0\": 2  # neutral\n",
    "    }\n",
    "}\n",
    "\n",
    "tasks = [\"main\"] + [\"nli\", \"senti\", \"normarg\"]\n",
    "\n",
    "\n",
    "cfg[\"task2classes\"] = {}\n",
    "for task in tasks:\n",
    "    if task == \"normarg\":\n",
    "        cfg[\"task2classes\"][\"normarg_polar\"] = cfg[\"tasks\"][\"normarg_polar\"][\"classes\"]\n",
    "        cfg[\"task2classes\"][\"normarg_jtype\"] = cfg[\"tasks\"][\"normarg_jtype\"][\"classes\"]\n",
    "        cfg[\"task2classes\"][\"normarg_norm_senti\"] = cfg[\"tasks\"][\"normarg_senti\"][\"classes\"]\n",
    "        cfg[\"task2classes\"][\"normarg_conseq_senti\"] = cfg[\"tasks\"][\"normarg_senti\"][\"classes\"]\n",
    "    else:\n",
    "        cfg[\"task2classes\"][task] = cfg[\"tasks\"][task][\"classes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS BLOCK IS DEPRECATED\n",
    "# This was our attempt to apply the logbert model for inference\n",
    "# Original code from here: https://github.com/yohanjo/tacl_arg_rel\n",
    "\n",
    "\n",
    "from models import *\n",
    "import os\n",
    "\n",
    "MODEL = 'trained.model'\n",
    "\n",
    "assert os.path.exists(MODEL), f\"Model not found: {MODEL}\"\n",
    "# Prepare the trainer\n",
    "trainer = Trainer(cfg)\n",
    "\n",
    "# Warmup with blend tasks\n",
    "init_model = torch.load(MODEL, map_location='cuda')\n",
    "trainer.init_model(init_model)\n",
    "trainer.model = trainer.model.to('cuda')\n",
    "\n",
    "# Prediction\n",
    "keys = [\"id\", \"con_text\", \"pre_text\", \"label_prob\", \"label_pred\"]\n",
    "\n",
    "print(f\"Loading data...\")\n",
    "# batches = trainer.make_batches(summPairs, [\"pre\", \"con\"]) \n",
    "batches = trainer.make_batches(pairs, [\"pre\", \"con\"])\n",
    "\n",
    "print(f\"Classifying {sum(len(b[\"id\"]) for b in batches)} replies over {len(batches)} batches:\")\n",
    "\n",
    "trainer.run_epoch(batches, \"predict\", \"main\")\n",
    "\n",
    "# for batch in batches:\n",
    "#     for i, id in enumerate(batch[\"id\"]):\n",
    "#         print(batch[\"pre_text\"][i][:20],'\\t',batch[\"con_text\"][i][:20],'\\t',batch[\"label_prob\"][i],'\\t',batch[\"label_pred\"][i])\n",
    "res = {ID: [b[\"label_pred\"][i], b[\"label_prob\"][i], b[\"pre_text\"][i], b[\"con_text\"][i], summs[ID][0]] for b in batches for i, ID in enumerate(b[\"id\"])}\n",
    "# for ID in res:\n",
    "#     print(f\"{ID}: {res[ID]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 21732.15it/s]\n",
      "Fetching 0 files: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Using LLaMA and prompt engineering to perform argumentative relation classification (ARC)\n",
    "\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "PATH = \"/home/bren/proj/F23_Rhetors/\"\n",
    "MODEL = \"llama-2-13b-chat.Q5_K_M.gguf\"\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/Llama-2-13B-chat-GGUF\",\n",
    "    model_file=f\"{PATH}{MODEL}\",\n",
    "    model_type=\"llama\",\n",
    "    context_length=2048,\n",
    "    gpu_layers=100,\n",
    "\tmax_new_tokens=64,\n",
    "    temperature=0.9,\n",
    ")\n",
    "\n",
    "for r in res:\n",
    "    prompt = f\"\"\"\n",
    "    P1:\n",
    "    {res[r][2]}\n",
    "    P2:\n",
    "    {res[r][3]}\n",
    "    \"\"\"\n",
    "    # print(prompt)\n",
    "\n",
    "\n",
    "    IN = f\"\"\"\n",
    "    [INST] <<SYS>>\n",
    "    Compare the statements made by P1 and P2. Does P2 AGREE or DISAGREE? Respond with your decision.\n",
    "    <</SYS>>\n",
    "    {prompt}[/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    res[r].append(str(llm(IN)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model results\n",
    "RES = None\n",
    "with open(\"res.json\", 'r') as f: RES = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LLaMA and prompt engineering to perform argumentative relation classification (ARC)\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=f\"{PATH}{MODEL}\",\n",
    "    n_gpu_layers=100,\n",
    "    n_batch=256,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "[INST] <<SYS>>\n",
    "Compare the statements made by P1 and P2. Does P2 AGREE or DISAGREE? Respond with your decision.\n",
    "<</SYS>>\n",
    "P1: {p1}\n",
    "P2: {p2}[/INST]\n",
    "\"\"\"\n",
    "\n",
    "p1 = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "p2 = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"p1\", \"p2\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "llm_chain.run({'p1':p1, 'p2':p2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populating the tree with valid argument flows based on argumentative relation classification results\n",
    "\n",
    "agrees = [\n",
    "    \"p2 agree\",\n",
    "    \"both\",\n",
    "    \"similar\"\n",
    "]\n",
    "\n",
    "disagrees = [\n",
    "    \"p2 disagree\",\n",
    "    \"disagrees\",\n",
    "    \"different opinion\",\n",
    "    \"different perspective\",\n",
    "    \"different views\",\n",
    "]\n",
    "\n",
    "vis = Digraph(format='png')\n",
    "\n",
    "def build(parID, cur):\n",
    "    # print(res[cur[\"id\"]])\n",
    "    g = int(25*(cur[\"threadScore\"]/largest*100)**(.25)+0)\n",
    "    \n",
    "    pred, prob, parReply, curReply, summ, outLLM = RES[cur[\"id\"]]\n",
    "    prob = [round(x,3) for x in prob]\n",
    "    resLLM = False if any(x in outLLM.lower() for x in disagrees) \\\n",
    "             else (True if any(x in outLLM.lower() for x in agrees) and \"disagree\" not in outLLM.lower() else True) # Default to agree\n",
    "\n",
    "    cur[\"agree\"] = True if (prob[0] > prob[1]) else False\n",
    "    resLLM = False if prob[1] > 0.75 else resLLM\n",
    "\n",
    "    # print(outLLM) if 'different' in outLLM and resLLM is True else None\n",
    "    content = textwrap.fill(cur[\"content\"], 25)[:200] + ('...' if len(cur[\"content\"]) > 200 else '')\n",
    "    outLLM = textwrap.fill(outLLM, 25)\n",
    "    summ = textwrap.fill(summ, 25)\n",
    "    label = f\"{cur['threadScore']:.2f} ({cur['score']:.2f}){f\"\\nReclassified {cur['agree']}→{resLLM}\" if cur['agree'] is not resLLM else \"\"} \\n{prob}\\n\\n{content}\" #\\n\\n{summ}\\n\\n{outLLM}\n",
    "    vis.node(cur[\"id\"], label=label, shape='box', style='filled', fillcolor=gradient[g].hex_l)\n",
    "\n",
    "    edge_color = 'black' if resLLM is None else ('green' if resLLM else 'red')\n",
    "    edge_label = '?' if resLLM is None else ('Agree' if resLLM else 'Disagree')\n",
    "\n",
    "    vis.edge(parID, cur[\"id\"], label=edge_label, color=edge_color, fontcolor=edge_color)\n",
    "    cur[\"agree\"] = resLLM\n",
    "\n",
    "    # Recurse for the replies\n",
    "    for r in cur[\"replies\"]:\n",
    "        build(cur[\"id\"], r)\n",
    "\n",
    "# Add a root node to the graph with the topic\n",
    "root = tree[\"root\"]\n",
    "vis.node(root[\"id\"], label=topic, shape='box')\n",
    "\n",
    "for r in root['replies']:\n",
    "    build(root[\"id\"], r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned comments:\n",
      "\t43 with low engagement\n",
      "\t26 were too short\n",
      "\t49 from shallow threads\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tree.png'"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F: Not sharing \"/usr/share/icons\" with sandbox: Path \"/usr\" is reserved by Flatpak\n"
     ]
    }
   ],
   "source": [
    "# Renders the current tree as it using graphviz\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4, sort_dicts=False)\n",
    "# pp.pprint(tree)\n",
    "json.dump(tree, open(\"tree.json\", \"w\"), indent=4, sort_keys=False)\n",
    "\n",
    "print(\n",
    "\t\"Pruned comments:\\n\"\n",
    "\tf\"\\t{pruned[0]} with low engagement\\n\"\n",
    "\tf\"\\t{pruned[1]} were too short\\n\"\n",
    "\tf\"\\t{pruned[2]} from shallow threads\\n\"\n",
    ")\n",
    "\n",
    "vis.render('tree', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pre.png'"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F: Not sharing \"/usr/share/icons\" with sandbox: Path \"/usr\" is reserved by Flatpak\n"
     ]
    }
   ],
   "source": [
    "# Rebuild tree without \"agree\" nodes and crudely recalculate scores\n",
    "\n",
    "# Load initial tree results\n",
    "pre = None\n",
    "with open(\"tree.json\", 'r') as f: pre = json.load(f)\n",
    "preVis = Digraph(format='png')\n",
    "\n",
    "def rescore(parID, cur):\n",
    "    # lookahead\n",
    "    loss = sum(r[\"threadScore\"] for r in cur[\"replies\"] if r[\"agree\"])\n",
    "    cur[\"replies\"] = [r for r in cur[\"replies\"] if not r[\"agree\"]]\n",
    "    # print(f\"-{loss}\") if loss != 0 else None\n",
    "\n",
    "    # Recurse for the replies\n",
    "    for r in cur[\"replies\"]:\n",
    "        loss += rescore(cur[\"id\"], r)\n",
    "\n",
    "    cur[\"threadScore\"] -= loss\n",
    "    cur[\"score\"] -= 0.5*loss\n",
    "\n",
    "    g = int(25*(cur[\"threadScore\"]/largest*100)**(.25)+0)\n",
    "\n",
    "    content = textwrap.fill(cur[\"content\"], 25)[:300] + ('...' if len(cur[\"content\"]) > 300 else '')\n",
    "    label = f\"{cur['threadScore']:.2f}{f\" (-{loss:.2f})\" if loss != 0 else \"\"}\\n\\n{content}\"\n",
    "    preVis.node(cur[\"id\"], label=label, shape='box', style='filled', fillcolor=gradient[g].hex_l)\n",
    "\n",
    "    edge_color = 'black' if cur[\"agree\"] is None else ('green' if cur[\"agree\"] else 'red')\n",
    "    edge_label = '?' if cur[\"agree\"] is None else ('Agree' if cur[\"agree\"] else 'Disagree')\n",
    "\n",
    "    preVis.edge(parID, cur[\"id\"], label=edge_label, color=edge_color, fontcolor=edge_color)\n",
    "\n",
    "    return loss\n",
    "\n",
    "preVis.node(root[\"id\"], label=topic, shape='box')\n",
    "root = pre[\"root\"]\n",
    "for tlc in root[\"replies\"]:\n",
    "    rescore(root[\"id\"], tlc)\n",
    "\n",
    "preVis.render('pre', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paths.png'"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F: Not sharing \"/usr/share/icons\" with sandbox: Path \"/usr\" is reserved by Flatpak\n"
     ]
    }
   ],
   "source": [
    "# Finding the paths and assigning each comment to either agent A or agent B\n",
    "\n",
    "paths = pre.copy()\n",
    "pathVis = Digraph(format='png')\n",
    "\n",
    "def findPaths(parID, cur):\n",
    "    if cur[\"agree\"] and parID != root[\"id\"]: return\n",
    "    if cur[\"depth\"] % 2 == 1: cur[\"agent\"] = 'A'\n",
    "    else: cur[\"agent\"] = 'B'\n",
    "    cur[\"replies\"].sort(key=lambda x: (0.6*x[\"threadScore\"]+0.4*x[\"score\"]), reverse=True)\n",
    "    g = int(25*(cur[\"threadScore\"]/largest*100)**(.25)+0)\n",
    "\n",
    "    \n",
    "    label = f\"{cur[\"agent\"]}:\\n\\n{textwrap.fill(cur[\"content\"], 50)[:2000]}{'...' if len(cur[\"content\"]) > 2000 else ''}\"\n",
    "    pathVis.node(cur[\"id\"], label=label, shape='box', style='filled', fillcolor=gradient[g].hex_l)\n",
    "\n",
    "    edge_color = 'black' if cur[\"agree\"] is None else ('green' if cur[\"agree\"] else 'red')\n",
    "    edge_label = '?' if cur[\"agree\"] is None else ('Agree' if cur[\"agree\"] else 'Disagree')\n",
    "\n",
    "    pathVis.edge(parID, cur[\"id\"], label=edge_label, color=edge_color, fontcolor=edge_color)\n",
    "\n",
    "    # Recurse for the replies\n",
    "    if cur[\"replies\"]:\n",
    "        # print(cur[\"replies\"])\n",
    "        findPaths(cur[\"id\"], cur[\"replies\"][0])\n",
    "\n",
    "\n",
    "pathVis.node(root[\"id\"], label=topic, shape='box')\n",
    "root = paths[\"root\"]\n",
    "root[\"replies\"].sort(key=lambda x: (0.6*x[\"threadScore\"]+0.4*x[\"score\"]), reverse=True)\n",
    "for tlc in root[\"replies\"][:6]: # explore top 6 threads\n",
    "    findPaths(root[\"id\"], tlc)\n",
    "    # print(tlc[\"threadScore\"])\n",
    "\n",
    "pathVis.render('paths', view=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a JSON script that can be fed into \"TTS_bark.ipynb\"\n",
    "\n",
    "exclude = [1]; debates = []\n",
    "\n",
    "def build(r):\n",
    "    return {\n",
    "        \"agent\": r[\"agent\"],\n",
    "        \"response\": r[\"content\"],\n",
    "        \"next\": build(r[\"replies\"][0]) if r[\"replies\"] else None\n",
    "\n",
    "    }\n",
    "\n",
    "for i, r in enumerate(root[\"replies\"]):\n",
    "    if i == 6: break\n",
    "    if i in exclude: continue\n",
    "    debates.append(build(r))\n",
    "\n",
    "script = {\n",
    "    \"topic\": topic,\n",
    "    \"debates\": debates\n",
    "}\n",
    "\n",
    "# pp.pprint(script)\n",
    "json.dump(script, open(\"script.json\", \"w\"), indent=4, sort_keys=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
